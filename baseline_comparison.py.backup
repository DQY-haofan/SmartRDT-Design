"""
Final Baseline Comparison Module v6.0
修复JSON序列化和权重问题
"""

import json
import numpy as np
import pandas as pd
from pymoo.core.problem import Problem
from pymoo.algorithms.soo.nonconvex.ga import GA
from pymoo.optimize import minimize
from pymoo.operators.sampling.rnd import FloatRandomSampling
from pymoo.operators.crossover.sbx import SBX
from pymoo.operators.mutation.pm import PM
from pymoo.termination import get_termination
import logging
from rdflib import Namespace
from tqdm import tqdm
from pymoo.core.callback import Callback

logger = logging.getLogger(__name__)

# Define namespace
RDTCO = Namespace("http://example.org/rdtco-maint#")


# Custom JSON encoder to handle numpy types (B1 fix)
class NumpyEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.bool_):
            return bool(obj)
        if isinstance(obj, np.integer):
            return int(obj)
        if isinstance(obj, np.floating):
            return float(obj)
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        return super(NumpyEncoder, self).default(obj)


class ProgressCallback(Callback):
    """进度条回调类"""
    def __init__(self, n_gen=100):
        super().__init__()
        self.pbar = None
        self.n_gen = n_gen

    def notify(self, algorithm):
        if self.pbar is None:
            self.pbar = tqdm(total=self.n_gen, desc="Weighted-Sum Optimization")

        self.pbar.update(1)
        self.pbar.set_postfix({
            'Gen': algorithm.n_gen,
            'Best': f"{algorithm.opt[0].F[0]:.4f}" if algorithm.opt else "N/A"
        })

        if algorithm.n_gen >= self.n_gen:
            self.pbar.close()


def run_greedy_cost_baseline(problem, ontology_graph):
    """
    Baseline 1: 贪心成本最小化策略
    对每个决策变量选择成本最低的选项
    """
    logger.info("Running Greedy Cost-Minimization Baseline...")

    # Initialize the solution vector
    x_greedy = np.zeros(problem.n_var)

    # Get mapper and evaluator
    mapper = problem.mapper
    evaluator = problem.evaluator

    # x1: Select cheapest sensor with reasonable performance
    sensor_costs = []
    for i, sensor in enumerate(mapper.sensor_options):
        query = f"""
        SELECT ?cost ?coverage WHERE {{
            <{sensor}> rdtco:hasInitialCost ?cost .
            OPTIONAL {{ <{sensor}> rdtco:hasCoverageEfficiency ?coverage }}
        }}
        """
        for row in ontology_graph.query(query, initNs={'rdtco': RDTCO}):
            if row.cost:
                cost = float(row.cost)
                coverage = float(row.coverage) if row.coverage else 1.0

                # Exclude sensors with very low coverage efficiency
                sensor_name = str(sensor).split('#')[-1]
                if 'IoT' not in sensor_name and coverage >= 0.5:
                    sensor_costs.append((i, cost, coverage))

    # Sort by cost, then by coverage (descending) as tiebreaker
    sensor_costs.sort(key=lambda x: (x[1], -x[2]))

    if sensor_costs:
        x_greedy[0] = sensor_costs[0][0]
    else:
        x_greedy[0] = 0  # Fallback

    # x2: Moderate data rate (balance between cost and performance)
    x_greedy[1] = 10.0  # 10 Hz is a reasonable compromise

    # x3, x3': Meso LOD (balanced detail and cost)
    x_greedy[2] = 1  # Meso
    x_greedy[3] = 1  # Meso

    # x4: Select cheapest algorithm with minimum performance
    algo_costs = []
    for i, algo in enumerate(mapper.algorithm_options):
        # Get algorithm recall rate
        recall_query = f"""
        SELECT ?recall ?procCost WHERE {{
            OPTIONAL {{ <{algo}> rdtco:hasRecall ?recall }}
            OPTIONAL {{ <{algo}> rdtco:hasProcessingCostPerGB ?procCost }}
        }}
        """

        algo_recall = 0.5  # Default
        proc_cost = 0.5  # Default

        for row in ontology_graph.query(recall_query, initNs={'rdtco': RDTCO}):
            if row.recall:
                algo_recall = float(row.recall)
            if row.procCost:
                proc_cost = float(row.procCost)

        # Only consider algorithms with recall >= 0.7 to avoid terrible performance
        if algo_recall >= 0.7:
            algo_costs.append((i, proc_cost, algo_recall))

    # Sort by cost, then by recall (descending) as tiebreaker
    algo_costs.sort(key=lambda x: (x[1], -x[2]))

    if algo_costs:
        x_greedy[4] = algo_costs[0][0]
    else:
        # Fallback: select first algorithm if none meet criteria
        x_greedy[4] = 0

    # x4_threshold: Middle value (no specific cost impact)
    x_greedy[5] = 0.5

    # x5: Select cheapest storage
    storage_costs = []
    for i, storage in enumerate(mapper.storage_options):
        query = f"""
        SELECT ?cost WHERE {{
            <{storage}> rdtco:hasInitialCost ?cost .
        }}
        """
        for row in ontology_graph.query(query, initNs={'rdtco': RDTCO}):
            if row.cost:
                storage_costs.append((i, float(row.cost)))

    storage_costs.sort(key=lambda x: x[1])
    x_greedy[6] = storage_costs[0][0]

    # x6: Select cheapest communication
    comm_costs = []
    for i, comm in enumerate(mapper.comm_options):
        query = f"""
        SELECT ?cost WHERE {{
            <{comm}> rdtco:hasInitialCost ?cost .
        }}
        """
        for row in ontology_graph.query(query, initNs={'rdtco': RDTCO}):
            if row.cost:
                comm_costs.append((i, float(row.cost)))

    comm_costs.sort(key=lambda x: x[1])
    x_greedy[7] = comm_costs[0][0]

    # x7: Select cheapest deployment
    deploy_costs = []
    for i, deploy in enumerate(mapper.deployment_options):
        query = f"""
        SELECT ?cost WHERE {{
            <{deploy}> rdtco:hasInitialCost ?cost .
        }}
        """
        for row in ontology_graph.query(query, initNs={'rdtco': RDTCO}):
            if row.cost:
                deploy_costs.append((i, float(row.cost)))

    deploy_costs.sort(key=lambda x: x[1])
    x_greedy[8] = deploy_costs[0][0]

    # x8: Small but reasonable crew size
    x_greedy[9] = 3  # 3 people is cost-effective

    # x9: Monthly inspection cycle (balance between cost and maintenance)
    x_greedy[10] = 30  # 30 days

    # Evaluate the greedy solution
    config = mapper.decode_solution(x_greedy)

    # Get raw objective values
    f1_raw = evaluator._calculate_total_cost_final(config)
    f2_raw = evaluator._calculate_detection_performance_final(config)
    f3_raw = evaluator._calculate_latency_final(config)
    f4_raw = evaluator._calculate_traffic_disruption_final(config)

    recall = 1 - f2_raw

    # Check constraints (B2 fix: use actual budget from config)
    g1 = f3_raw - evaluator.config['max_latency_seconds']
    g2 = evaluator.config['min_recall_threshold'] - recall
    g3 = f1_raw - evaluator.config['budget_cap_usd']

    result = {
        'method': 'Greedy Cost-Minimization',
        'configuration': {
            'sensor': str(config['sensor']).split('/')[-1],
            'data_rate_Hz': float(config['data_rate']),
            'geometric_LOD': config['geo_lod'],
            'condition_LOD': config['cond_lod'],
            'algorithm': str(config['algorithm']).split('/')[-1],
            'detection_threshold': float(config['detection_threshold']),
            'storage': str(config['storage']).split('/')[-1],
            'communication': str(config['communication']).split('/')[-1],
            'deployment': str(config['deployment']).split('/')[-1],
            'crew_size': int(config['crew_size']),
            'inspection_cycle_days': int(config['inspection_cycle'])
        },
        'objectives': {
            'f1_total_cost_USD': float(f1_raw),
            'f2_one_minus_recall': float(f2_raw),
            'f3_latency_seconds': float(f3_raw),
            'f4_traffic_disruption_hours': float(f4_raw),
            'detection_recall': float(recall)
        },
        'constraints': {
            'g1_latency_violation': float(g1),
            'g2_recall_violation': float(g2),
            'g3_budget_violation': float(g3),
            'is_feasible': bool(g1 <= 0 and g2 <= 0 and g3 <= 0)
        },
        'x_vector': x_greedy.tolist()
    }

    # Save result with custom encoder
    with open('./results/baseline_greedy_solution.json', 'w') as f:
        json.dump(result, f, indent=2, cls=NumpyEncoder)

    logger.info(f"Greedy baseline complete: Cost=${f1_raw:.0f}, Recall={recall:.3f}, "
                f"Latency={f3_raw:.1f}s, Feasible={result['constraints']['is_feasible']}")

    return result


def run_weighted_sum_baseline(problem, ontology_graph):
    """
    Baseline 2: 加权和单目标优化
    将多目标通过固定权重转化为单目标
    """
    logger.info("Running Weighted-Sum Single-Objective Baseline...")

    # C4 fix: Adjust weights to prioritize recall more
    weights = {
        'cost': 0.5,     # Reduced from 0.6
        'recall': 0.3,   # Increased from 0.2
        'latency': 0.1,
        'disruption': 0.1
    }

    class WeightedSumProblem(Problem):
        def __init__(self, original_problem, weights):
            self.original_problem = original_problem
            self.weights = weights
            self.mapper = original_problem.mapper
            self.evaluator = original_problem.evaluator

            super().__init__(
                n_var=original_problem.n_var,
                n_obj=1,  # Single objective
                n_constr=original_problem.n_constr,
                xl=original_problem.xl,
                xu=original_problem.xu,
                type_var=original_problem.type_var
            )

        def _evaluate(self, X, out, *args, **kwargs):
            objectives = []
            constraints = []

            for x in X:
                # Handle integer variables
                x_copy = x.copy()
                for i in range(len(x_copy)):
                    if self.original_problem.var_types[i] == 'int':
                        x_copy[i] = int(np.round(x_copy[i]))
                        x_copy[i] = np.clip(x_copy[i], self.xl[i], self.xu[i])

                # Get normalized objectives from original evaluator
                obj_values = self.evaluator.evaluate(x_copy)

                # Calculate weighted sum
                weighted_sum = (self.weights['cost'] * obj_values[0] +
                               self.weights['recall'] * obj_values[1] +
                               self.weights['latency'] * obj_values[2] +
                               self.weights['disruption'] * obj_values[3])

                objectives.append([weighted_sum])

                # Calculate constraints
                raw_values = self.evaluator.get_raw_objectives(x_copy)
                f1_raw = raw_values[0]
                f2_raw = raw_values[1]
                f3_raw = raw_values[2]
                recall = 1 - f2_raw

                g1 = f3_raw - self.evaluator.config['max_latency_seconds']
                g2 = self.evaluator.config['min_recall_threshold'] - recall
                g3 = f1_raw - self.evaluator.config['budget_cap_usd']

                # C4 fix: Add penalty for recall constraint violation
                if g2 > 0:
                    weighted_sum += g2 * 10  # Heavy penalty for recall violation

                constraints.append([g1, g2, g3])

            out["F"] = np.array(objectives)
            out["G"] = np.array(constraints)

    # Create weighted sum problem
    ws_problem = WeightedSumProblem(problem, weights)

    # Configure single-objective GA
    algorithm = GA(
        pop_size=100,
        sampling=FloatRandomSampling(),
        crossover=SBX(eta=15, prob=0.9),
        mutation=PM(eta=20, prob=1.0/ws_problem.n_var),
        eliminate_duplicates=True
    )

    # Run optimization with progress bar
    n_generations = 10
    termination = get_termination("n_gen", n_generations)

    # Add progress callback
    progress_callback = ProgressCallback(n_gen=n_generations)

    res = minimize(
        ws_problem,
        algorithm,
        termination,
        seed=42,
        verbose=False,
        callback=progress_callback
    )

    # Get the best solution
    x_weighted = res.X

    # Evaluate full objectives
    config = problem.mapper.decode_solution(x_weighted)

    f1_raw = problem.evaluator._calculate_total_cost_final(config)
    f2_raw = problem.evaluator._calculate_detection_performance_final(config)
    f3_raw = problem.evaluator._calculate_latency_final(config)
    f4_raw = problem.evaluator._calculate_traffic_disruption_final(config)

    recall = 1 - f2_raw

    # Check constraints
    g1 = f3_raw - problem.evaluator.config['max_latency_seconds']
    g2 = problem.evaluator.config['min_recall_threshold'] - recall
    g3 = f1_raw - problem.evaluator.config['budget_cap_usd']

    result = {
        'method': 'Weighted-Sum Optimization',
        'weights': weights,
        'configuration': {
            'sensor': str(config['sensor']).split('/')[-1],
            'data_rate_Hz': float(config['data_rate']),
            'geometric_LOD': config['geo_lod'],
            'condition_LOD': config['cond_lod'],
            'algorithm': str(config['algorithm']).split('/')[-1],
            'detection_threshold': float(config['detection_threshold']),
            'storage': str(config['storage']).split('/')[-1],
            'communication': str(config['communication']).split('/')[-1],
            'deployment': str(config['deployment']).split('/')[-1],
            'crew_size': int(config['crew_size']),
            'inspection_cycle_days': int(config['inspection_cycle'])
        },
        'objectives': {
            'f1_total_cost_USD': float(f1_raw),
            'f2_one_minus_recall': float(f2_raw),
            'f3_latency_seconds': float(f3_raw),
            'f4_traffic_disruption_hours': float(f4_raw),
            'detection_recall': float(recall)
        },
        'constraints': {
            'g1_latency_violation': float(g1),
            'g2_recall_violation': float(g2),
            'g3_budget_violation': float(g3),
            'is_feasible': bool(g1 <= 0 and g2 <= 0 and g3 <= 0)
        },
        'weighted_sum_score': float(res.F[0]),
        'x_vector': x_weighted.tolist()
    }

    # Save result with custom encoder
    with open('./results/baseline_weighted_sum_solution.json', 'w') as f:
        json.dump(result, f, indent=2, cls=NumpyEncoder)

    logger.info(f"Weighted-sum baseline complete: Cost=${f1_raw:.0f}, Recall={recall:.3f}, "
                f"Latency={f3_raw:.1f}s, Feasible={result['constraints']['is_feasible']}")

    return result


def run_all_baselines(problem, ontology_graph):
    """运行所有基准方法"""
    results = {}

    # Run greedy baseline
    greedy_result = run_greedy_cost_baseline(problem, ontology_graph)
    results['greedy'] = greedy_result

    # Run weighted sum baseline
    weighted_result = run_weighted_sum_baseline(problem, ontology_graph)
    results['weighted_sum'] = weighted_result

    # Save combined results with custom encoder
    with open('./results/all_baseline_results.json', 'w') as f:
        json.dump(results, f, indent=2, cls=NumpyEncoder)

    logger.info("All baseline comparisons complete")

    return results


def run_ablation_study(problem, ontology_graph):
    """
    Ablation Study: 运行没有语义预过滤的NSGA-II
    """
    logger.info("Running Ablation Study (No Semantic Pre-filtering)...")

    from pymoo.algorithms.moo.nsga2 import NSGA2

    class AblationProgressCallback(Callback):
        """Ablation study进度条回调类"""
        def __init__(self, n_gen=10):
            super().__init__()
            self.pbar = None
            self.n_gen = n_gen

        def notify(self, algorithm):
            if self.pbar is None:
                self.pbar = tqdm(total=self.n_gen, desc="Ablation Study")

            self.pbar.update(1)

            # Calculate feasibility statistics
            if hasattr(algorithm, 'pop') and algorithm.pop is not None:
                G = algorithm.pop.get("G")
                if G is not None:
                    n_feasible = np.sum([np.all(g <= 0) for g in G])
                    cv_avg = np.mean([np.sum(np.maximum(0, g)) for g in G])
                    self.pbar.set_postfix({
                        'Gen': algorithm.n_gen,
                        'Feasible': n_feasible,
                        'CV_avg': f"{cv_avg:.3f}"
                    })

            if algorithm.n_gen >= self.n_gen:
                self.pbar.close()

    # Configure NSGA-II without semantic pre-filtering
    algorithm = NSGA2(
        pop_size=200,
        sampling=FloatRandomSampling(),  # Pure random sampling
        crossover=SBX(eta=15, prob=0.9),
        mutation=PM(eta=20, prob=1.0/problem.n_var),
        eliminate_duplicates=True
    )

    # Run optimization with progress bar
    n_generations = 10
    termination = get_termination("n_gen", n_generations)

    # Add progress callback
    progress_callback = AblationProgressCallback(n_gen=n_generations)

    res = minimize(
        problem,
        algorithm,
        termination,
        seed=42,
        save_history=True,
        verbose=False,
        callback=progress_callback
    )

    # Analyze convergence history
    convergence_data = []
    for gen in range(len(res.history)):
        opt = res.history[gen].opt
        if opt is not None and len(opt) > 0:
            # Average constraint violation
            cv_avg = np.mean([np.sum(np.maximum(0, g)) for g in opt.get("G")])
            # Number of feasible solutions
            n_feasible = np.sum([np.all(g <= 0) for g in opt.get("G")])

            convergence_data.append({
                'generation': gen,
                'cv_average': float(cv_avg),
                'n_feasible': int(n_feasible),
                'n_total': len(opt)
            })

    # Save ablation study results
    ablation_result = {
        'method': 'NSGA-II without Semantic Pre-filtering',
        'convergence_history': convergence_data,
        'final_statistics': {
            'n_solutions': len(res.X) if res.X is not None else 0,
            'n_feasible': np.sum([np.all(g <= 0) for g in res.G]) if res.G is not None else 0,
            'generations_to_first_feasible': next((d['generation'] for d in convergence_data if d['n_feasible'] > 0), -1)
        }
    }

    with open('./results/ablation_study_results.json', 'w') as f:
        json.dump(ablation_result, f, indent=2, cls=NumpyEncoder)

    logger.info(f"Ablation study complete: {ablation_result['final_statistics']['n_feasible']} "
                f"feasible solutions found")

    return ablation_result